# About

This data pipeline collects recipes from other sites, transforms them with ML techniques, then uploads them to an SQL database. To allow for live deployment, data has been hosted with AWS. To run this yourself, you will need to set up the AWS services and provide the access keys to airflow. Refer to 'Running the Pipeline' for more detailed instructions.

# How it works

There are four main transformations that the data goes through.

- First, the pipeline queries chatGPT to improve the recipe's instructions by inserting ingredient quantities and generates additional content like tags, descriptions, and an image prompt.
- Second, a simple Python string processing function combines all the ingredients from the original source, which separates ingredients into sections. This will be useful in the future for calculating recipe prices and other advanced use cases.
- Third, the pipeline generates a recipe image by using the image prompt generated earlier and calling a stable diffusion API. After experimentation, the images produced tend to be accurate and professional-looking, with occasional minor inconsistencies (e.g., multiple handles on a pot).
- Fourth, an embedding of the recipe is generated by calculating a GloVe embedding for each tag and averaging the results.

# Running the Pipeline

To run the pipeline, follow these steps:

1. Make sure Docker is installed on your machine by visiting https://docs.docker.com/get-docker/.
2. Pull the Apache Airflow Docker image by running the following command: `docker pull apache/airflow`
3. Build the extended image in the airflow-datapipeline directory by running this command: `docker build . -t chef_airflow:latest`
4. Run the Docker image by running this command: `docker compose up`
5. Once the image is running, you can use the Airflow UI by navigating to localhost:8080 in a web browser and using the default credentials (user=airflow, password=airflow).
6. To set up environment variables, navigate to Admin/Variables in the Airflow UI, and import your own keys by using the variables.json file provided in the /envs folder.
7. To set up a connection, navigate to Admin/Connections in the Airflow UI and manually enter your details into the connection pane. An example of a connection file, mongo.json, is provided in the /envs folder to show what information is needed and in what form.
